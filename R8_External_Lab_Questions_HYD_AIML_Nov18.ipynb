{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R8_External_Lab_Questions-HYD_AIML_Nov18.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QGIsF1ADyJ58"
      },
      "source": [
        "# Transfer Learning CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E-n6tVFayGBe"
      },
      "source": [
        "* Train a simple convnet on the CIFAR dataset the first 5 output classes [0..4].\n",
        "* Freeze convolutional layers and fine-tune dense layers for the last 5 ouput classes [5..9].\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cq8ejXHJyGYq"
      },
      "source": [
        "### 1. Import CIFAR10 data and create 2 datasets with one dataset having classes from 0 to 4 and other having classes from 5 to 9 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uWYbxnBayFUP",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams['figure.figsize'] = (15, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fp756_OInLtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Zwz0jPqzIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = []\n",
        "y_train_num = []\n",
        "x_test = []\n",
        "y_test_num = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCoz4CitnO_D",
        "colab_type": "code",
        "outputId": "5f6c3618-cdb0-4602-ca3a-f95fec9fc13e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "%matplotlib inline\n",
        "# Load/Prep the Data\n",
        "(x_train, y_train_num), (x_test, y_test_num) = cifar10.load_data()\n",
        "#x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
        "#x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
        "#x_train /= 255\n",
        "#x_test /= 255\n",
        "#y_train = np_utils.to_categorical(y_train_num, 10)\n",
        "#y_test = np_utils.to_categorical(y_test_num, 10)\n",
        "\n",
        "print('--- THE DATA ---')\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train_num shape:', y_train_num.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- THE DATA ---\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "y_train_num shape: (50000, 1)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anh-hC9PusnS",
        "colab_type": "code",
        "outputId": "88546e95-0030-43e9-c9a8-6db02a0974b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "x_train"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[ 59,  62,  63],\n",
              "         [ 43,  46,  45],\n",
              "         [ 50,  48,  43],\n",
              "         ...,\n",
              "         [158, 132, 108],\n",
              "         [152, 125, 102],\n",
              "         [148, 124, 103]],\n",
              "\n",
              "        [[ 16,  20,  20],\n",
              "         [  0,   0,   0],\n",
              "         [ 18,   8,   0],\n",
              "         ...,\n",
              "         [123,  88,  55],\n",
              "         [119,  83,  50],\n",
              "         [122,  87,  57]],\n",
              "\n",
              "        [[ 25,  24,  21],\n",
              "         [ 16,   7,   0],\n",
              "         [ 49,  27,   8],\n",
              "         ...,\n",
              "         [118,  84,  50],\n",
              "         [120,  84,  50],\n",
              "         [109,  73,  42]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[208, 170,  96],\n",
              "         [201, 153,  34],\n",
              "         [198, 161,  26],\n",
              "         ...,\n",
              "         [160, 133,  70],\n",
              "         [ 56,  31,   7],\n",
              "         [ 53,  34,  20]],\n",
              "\n",
              "        [[180, 139,  96],\n",
              "         [173, 123,  42],\n",
              "         [186, 144,  30],\n",
              "         ...,\n",
              "         [184, 148,  94],\n",
              "         [ 97,  62,  34],\n",
              "         [ 83,  53,  34]],\n",
              "\n",
              "        [[177, 144, 116],\n",
              "         [168, 129,  94],\n",
              "         [179, 142,  87],\n",
              "         ...,\n",
              "         [216, 184, 140],\n",
              "         [151, 118,  84],\n",
              "         [123,  92,  72]]],\n",
              "\n",
              "\n",
              "       [[[154, 177, 187],\n",
              "         [126, 137, 136],\n",
              "         [105, 104,  95],\n",
              "         ...,\n",
              "         [ 91,  95,  71],\n",
              "         [ 87,  90,  71],\n",
              "         [ 79,  81,  70]],\n",
              "\n",
              "        [[140, 160, 169],\n",
              "         [145, 153, 154],\n",
              "         [125, 125, 118],\n",
              "         ...,\n",
              "         [ 96,  99,  78],\n",
              "         [ 77,  80,  62],\n",
              "         [ 71,  73,  61]],\n",
              "\n",
              "        [[140, 155, 164],\n",
              "         [139, 146, 149],\n",
              "         [115, 115, 112],\n",
              "         ...,\n",
              "         [ 79,  82,  64],\n",
              "         [ 68,  70,  55],\n",
              "         [ 67,  69,  55]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[175, 167, 166],\n",
              "         [156, 154, 160],\n",
              "         [154, 160, 170],\n",
              "         ...,\n",
              "         [ 42,  34,  36],\n",
              "         [ 61,  53,  57],\n",
              "         [ 93,  83,  91]],\n",
              "\n",
              "        [[165, 154, 128],\n",
              "         [156, 152, 130],\n",
              "         [159, 161, 142],\n",
              "         ...,\n",
              "         [103,  93,  96],\n",
              "         [123, 114, 120],\n",
              "         [131, 121, 131]],\n",
              "\n",
              "        [[163, 148, 120],\n",
              "         [158, 148, 122],\n",
              "         [163, 156, 133],\n",
              "         ...,\n",
              "         [143, 133, 139],\n",
              "         [143, 134, 142],\n",
              "         [143, 133, 144]]],\n",
              "\n",
              "\n",
              "       [[[255, 255, 255],\n",
              "         [253, 253, 253],\n",
              "         [253, 253, 253],\n",
              "         ...,\n",
              "         [253, 253, 253],\n",
              "         [253, 253, 253],\n",
              "         [253, 253, 253]],\n",
              "\n",
              "        [[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255]],\n",
              "\n",
              "        [[255, 255, 255],\n",
              "         [254, 254, 254],\n",
              "         [254, 254, 254],\n",
              "         ...,\n",
              "         [254, 254, 254],\n",
              "         [254, 254, 254],\n",
              "         [254, 254, 254]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[113, 120, 112],\n",
              "         [111, 118, 111],\n",
              "         [105, 112, 106],\n",
              "         ...,\n",
              "         [ 72,  81,  80],\n",
              "         [ 72,  80,  79],\n",
              "         [ 72,  80,  79]],\n",
              "\n",
              "        [[111, 118, 110],\n",
              "         [104, 111, 104],\n",
              "         [ 99, 106,  98],\n",
              "         ...,\n",
              "         [ 68,  75,  73],\n",
              "         [ 70,  76,  75],\n",
              "         [ 78,  84,  82]],\n",
              "\n",
              "        [[106, 113, 105],\n",
              "         [ 99, 106,  98],\n",
              "         [ 95, 102,  94],\n",
              "         ...,\n",
              "         [ 78,  85,  83],\n",
              "         [ 79,  85,  83],\n",
              "         [ 80,  86,  84]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[ 35, 178, 235],\n",
              "         [ 40, 176, 239],\n",
              "         [ 42, 176, 241],\n",
              "         ...,\n",
              "         [ 99, 177, 219],\n",
              "         [ 79, 147, 197],\n",
              "         [ 89, 148, 189]],\n",
              "\n",
              "        [[ 57, 182, 234],\n",
              "         [ 44, 184, 250],\n",
              "         [ 50, 183, 240],\n",
              "         ...,\n",
              "         [156, 182, 200],\n",
              "         [141, 177, 206],\n",
              "         [116, 149, 175]],\n",
              "\n",
              "        [[ 98, 197, 237],\n",
              "         [ 64, 189, 252],\n",
              "         [ 69, 192, 245],\n",
              "         ...,\n",
              "         [188, 195, 206],\n",
              "         [119, 135, 147],\n",
              "         [ 61,  79,  90]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 73,  79,  77],\n",
              "         [ 53,  63,  68],\n",
              "         [ 54,  68,  80],\n",
              "         ...,\n",
              "         [ 17,  40,  64],\n",
              "         [ 21,  36,  51],\n",
              "         [ 33,  48,  49]],\n",
              "\n",
              "        [[ 61,  68,  75],\n",
              "         [ 55,  70,  86],\n",
              "         [ 57,  79, 103],\n",
              "         ...,\n",
              "         [ 24,  48,  72],\n",
              "         [ 17,  35,  53],\n",
              "         [  7,  23,  32]],\n",
              "\n",
              "        [[ 44,  56,  73],\n",
              "         [ 46,  66,  88],\n",
              "         [ 49,  77, 105],\n",
              "         ...,\n",
              "         [ 27,  52,  77],\n",
              "         [ 21,  43,  66],\n",
              "         [ 12,  31,  50]]],\n",
              "\n",
              "\n",
              "       [[[189, 211, 240],\n",
              "         [186, 208, 236],\n",
              "         [185, 207, 235],\n",
              "         ...,\n",
              "         [175, 195, 224],\n",
              "         [172, 194, 222],\n",
              "         [169, 194, 220]],\n",
              "\n",
              "        [[194, 210, 239],\n",
              "         [191, 207, 236],\n",
              "         [190, 206, 235],\n",
              "         ...,\n",
              "         [173, 192, 220],\n",
              "         [171, 191, 218],\n",
              "         [167, 190, 216]],\n",
              "\n",
              "        [[208, 219, 244],\n",
              "         [205, 216, 240],\n",
              "         [204, 215, 239],\n",
              "         ...,\n",
              "         [175, 191, 217],\n",
              "         [172, 190, 216],\n",
              "         [169, 191, 215]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[207, 199, 181],\n",
              "         [203, 195, 175],\n",
              "         [203, 196, 173],\n",
              "         ...,\n",
              "         [135, 132, 127],\n",
              "         [162, 158, 150],\n",
              "         [168, 163, 151]],\n",
              "\n",
              "        [[198, 190, 170],\n",
              "         [189, 181, 159],\n",
              "         [180, 172, 147],\n",
              "         ...,\n",
              "         [178, 171, 160],\n",
              "         [175, 169, 156],\n",
              "         [175, 169, 154]],\n",
              "\n",
              "        [[198, 189, 173],\n",
              "         [189, 181, 162],\n",
              "         [178, 170, 149],\n",
              "         ...,\n",
              "         [195, 184, 169],\n",
              "         [196, 189, 171],\n",
              "         [195, 190, 171]]],\n",
              "\n",
              "\n",
              "       [[[229, 229, 239],\n",
              "         [236, 237, 247],\n",
              "         [234, 236, 247],\n",
              "         ...,\n",
              "         [217, 219, 233],\n",
              "         [221, 223, 234],\n",
              "         [222, 223, 233]],\n",
              "\n",
              "        [[222, 221, 229],\n",
              "         [239, 239, 249],\n",
              "         [233, 234, 246],\n",
              "         ...,\n",
              "         [223, 223, 236],\n",
              "         [227, 228, 238],\n",
              "         [210, 211, 220]],\n",
              "\n",
              "        [[213, 206, 211],\n",
              "         [234, 232, 239],\n",
              "         [231, 233, 244],\n",
              "         ...,\n",
              "         [220, 220, 232],\n",
              "         [220, 219, 232],\n",
              "         [202, 203, 215]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[150, 143, 135],\n",
              "         [140, 135, 127],\n",
              "         [132, 127, 120],\n",
              "         ...,\n",
              "         [224, 222, 218],\n",
              "         [230, 228, 225],\n",
              "         [241, 241, 238]],\n",
              "\n",
              "        [[137, 132, 126],\n",
              "         [130, 127, 120],\n",
              "         [125, 121, 115],\n",
              "         ...,\n",
              "         [181, 180, 178],\n",
              "         [202, 201, 198],\n",
              "         [212, 211, 207]],\n",
              "\n",
              "        [[122, 119, 114],\n",
              "         [118, 116, 110],\n",
              "         [120, 116, 111],\n",
              "         ...,\n",
              "         [179, 177, 173],\n",
              "         [164, 164, 162],\n",
              "         [163, 163, 161]]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBLBoULpopX_",
        "colab_type": "code",
        "outputId": "07f3911a-67a3-4bc7-b1b8-35c11b169df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "y_train_num"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6],\n",
              "       [9],\n",
              "       [9],\n",
              "       ...,\n",
              "       [9],\n",
              "       [1],\n",
              "       [1]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFf4xgXsr5Rh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_examples = 50000\n",
        "\n",
        "x_train_lt5 = []\n",
        "y_train_lt5 = []\n",
        "x_train_gt5 = []\n",
        "y_train_gt5 = []\n",
        "\n",
        "x_test_lt5 = []\n",
        "y_test_lt5 = []\n",
        "x_test_gt5 = []\n",
        "y_test_gt5 = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fanY2sGCXi88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for ix in range(n_examples):\n",
        "    if y_train_num[ix] < 5:\n",
        "        # put data in set 1\n",
        "        x_train_lt5.append(x_train[ix]/255.0)\n",
        "        y_train_lt5.append(y_train_num[ix])\n",
        "    else:\n",
        "        # put data in set 2\n",
        "        x_train_gt5.append(x_train[ix]/255.0)\n",
        "        y_train_gt5.append(y_train_num[ix])\n",
        "\n",
        "for ix in range(y_test.shape[0]):\n",
        "    if y_test_num[ix] < 5:\n",
        "        # put data in set 1\n",
        "        x_test_lt5.append(x_test[ix]/255.0)\n",
        "        y_test_lt5.append(y_test_num[ix])\n",
        "    else:\n",
        "        # put data in set 2\n",
        "        x_test_gt5.append(x_test[ix]/255.0)\n",
        "        y_test_gt5.append(y_test_num[ix])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xtCKmQh4yXhT"
      },
      "source": [
        "### 2. Use One-hot encoding to divide y_train and y_test into required no of output classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB4YV69vXfF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_lt5 = np_utils.to_categorical(y_train_lt5, 5)\n",
        "y_test_lt5 = np_utils.to_categorical(y_test_lt5, 5)\n",
        "\n",
        "y_train_gt5 = np_utils.to_categorical(y_train_gt5, 10)\n",
        "y_test_gt5 = np_utils.to_categorical(y_test_gt5, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cuOiKWfeybAl"
      },
      "source": [
        "### 3. Build a sequential neural network model which can classify the classes 0 to 4 of CIFAR10 dataset with at least 80% accuracy on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVpSUiFprxB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_lt5 = np.asarray(x_train_lt5).reshape((-1, 32, 32, 3))\n",
        "x_test_lt5 = np.asarray(x_test_lt5).reshape((-1, 32, 32, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5HzxNbiiyoBD",
        "outputId": "6bd007bb-4928-4f11-e328-bfac2dcfe6d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Define Model\n",
        "model = Sequential()\n",
        "\n",
        "# 1st Conv Layer\n",
        "model.add(Convolution2D(32, 5, 5, input_shape=(32, 32, 3)))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# 2nd Conv Layer\n",
        "model.add(Convolution2D(16, 5, 5))\n",
        "model.add(Activation('relu'))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), input_shape=(32, 32, 3...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (5, 5))`\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7erj4nI0nbn3",
        "colab_type": "code",
        "outputId": "c7a8b787-6bec-4bf0-c088-fb05162b3112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "# Dropout\n",
        "#model.add(Dropout(0.1))\n",
        "\n",
        "# 3rd Conv Layer\n",
        "model.add(Convolution2D(8, 3, 3, activation='relu'))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\")`\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSLNl5sNnnSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fully Connected Layer\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "    \n",
        "# More Dropout\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "# Prediction Layer\n",
        "model.add(Dense(5))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-E6ou4Znzht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx9Medf8rmTj",
        "colab_type": "code",
        "outputId": "28cbd623-31ea-43bc-efc4-365fff0c505d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('X_train shape:', x_train_lt5.shape)\n",
        "print('X_test shape:', x_test_lt5.shape)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (25000, 32, 32, 3)\n",
            "X_test shape: (5000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAmU0XIZ5KKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, with_statement, division\n",
        "import torch\n",
        "from tqdm.autonotebook import tqdm\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from matplotlib import pyplot\n",
        "from pandas import DataFrame\n",
        "import torchvision.datasets as dset\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import random\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "\n",
        "class LRFinder(object):\n",
        "    \"\"\"Learning rate range test.\n",
        "\n",
        "    The learning rate range test increases the learning rate in a pre-training run\n",
        "    between two boundaries in a linear or exponential manner. It provides valuable\n",
        "    information on how well the network can be trained over a range of learning rates\n",
        "    and what is the optimal learning rate.\n",
        "\n",
        "    Arguments:\n",
        "        model (torch.nn.Module): wrapped model.\n",
        "        optimizer (torch.optim.Optimizer): wrapped optimizer where the defined learning\n",
        "            is assumed to be the lower boundary of the range test.\n",
        "        criterion (torch.nn.Module): wrapped loss function.\n",
        "        device (str or torch.device, optional): a string (\"cpu\" or \"cuda\") with an\n",
        "            optional ordinal for the device type (e.g. \"cuda:X\", where is the ordinal).\n",
        "            Alternatively, can be an object representing the device on which the\n",
        "            computation will take place. Default: None, uses the same device as `model`.\n",
        "\n",
        "    Example:\n",
        "        >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n",
        "        >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n",
        "\n",
        "    Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
        "    fastai/lr_find: https://github.com/fastai/fastai\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, criterion, device=None):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.history = {\"lr\": [], \"loss\": []}\n",
        "        self.best_loss = None\n",
        "\n",
        "        # Save the original state of the model and optimizer so they can be restored if\n",
        "        # needed\n",
        "        self.model_state = model.state_dict()\n",
        "        self.model_device = next(self.model.parameters()).device\n",
        "        self.optimizer_state = optimizer.state_dict()\n",
        "\n",
        "        # If device is None, use the same as the model\n",
        "        if device:\n",
        "            self.device = device\n",
        "        else:\n",
        "            self.device = self.model_device\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Restores the model and optimizer to their initial states.\"\"\"\n",
        "        self.model.load_state_dict(self.model_state)\n",
        "        self.model.to(self.model_device)\n",
        "        self.optimizer.load_state_dict(self.optimizer_state)\n",
        "\n",
        "    def range_test(\n",
        "        self,\n",
        "        train_loader,\n",
        "        val_loader=None,\n",
        "        end_lr=10,\n",
        "        num_iter=100,\n",
        "        step_mode=\"exp\",\n",
        "        smooth_f=0.05,\n",
        "        diverge_th=5,\n",
        "    ):\n",
        "        \"\"\"Performs the learning rate range test.\n",
        "\n",
        "        Arguments:\n",
        "            train_loader (torch.utils.data.DataLoader): the training set data laoder.\n",
        "            val_loader (torch.utils.data.DataLoader, optional): if `None` the range test\n",
        "                will only use the training loss. When given a data loader, the model is\n",
        "                evaluated after each iteration on that dataset and the evaluation loss\n",
        "                is used. Note that in this mode the test takes significantly longer but\n",
        "                generally produces more precise results. Default: None.\n",
        "            end_lr (float, optional): the maximum learning rate to test. Default: 10.\n",
        "            num_iter (int, optional): the number of iterations over which the test\n",
        "                occurs. Default: 100.\n",
        "            step_mode (str, optional): one of the available learning rate policies,\n",
        "                linear or exponential (\"linear\", \"exp\"). Default: \"exp\".\n",
        "            smooth_f (float, optional): the loss smoothing factor within the [0, 1[\n",
        "                interval. Disabled if set to 0, otherwise the loss is smoothed using\n",
        "                exponential smoothing. Default: 0.05.\n",
        "            diverge_th (int, optional): the test is stopped when the loss surpasses the\n",
        "                threshold:  diverge_th * best_loss. Default: 5.\n",
        "\n",
        "        \"\"\"\n",
        "        # Reset test results\n",
        "        self.history = {\"lr\": [], \"loss\": []}\n",
        "        self.best_loss = None\n",
        "\n",
        "        # Move the model to the proper device\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Initialize the proper learning rate policy\n",
        "        if step_mode.lower() == \"exp\":\n",
        "            lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
        "        elif step_mode.lower() == \"linear\":\n",
        "            lr_schedule = LinearLR(self.optimizer, end_lr, num_iter)\n",
        "        else:\n",
        "            raise ValueError(\"expected one of (exp, linear), got {}\".format(step_mode))\n",
        "\n",
        "        if smooth_f < 0 or smooth_f >= 1:\n",
        "            raise ValueError(\"smooth_f is outside the range [0, 1[\")\n",
        "\n",
        "        # Create an iterator to get data batch by batch\n",
        "        iterator = iter(train_loader)\n",
        "        for iteration in tqdm(range(num_iter)):\n",
        "            # Get a new set of inputs and labels\n",
        "            try:\n",
        "                inputs, labels = next(iterator)\n",
        "            except StopIteration:\n",
        "                iterator = iter(train_loader)\n",
        "                inputs, labels = next(iterator)\n",
        "\n",
        "            # Train on batch and retrieve loss\n",
        "            loss = self._train_batch(inputs, labels)\n",
        "            if val_loader:\n",
        "                loss = self._validate(val_loader)\n",
        "\n",
        "            # Update the learning rate\n",
        "            lr_schedule.step()\n",
        "            self.history[\"lr\"].append(lr_schedule.get_lr()[0])\n",
        "\n",
        "            # Track the best loss and smooth it if smooth_f is specified\n",
        "            if iteration == 0:\n",
        "                self.best_loss = loss\n",
        "            else:\n",
        "                if smooth_f > 0:\n",
        "                    loss = smooth_f * loss + (1 - smooth_f) * self.history[\"loss\"][-1]\n",
        "                if loss < self.best_loss:\n",
        "                    self.best_loss = loss\n",
        "\n",
        "            # Check if the loss has diverged; if it has, stop the test\n",
        "            self.history[\"loss\"].append(loss)\n",
        "            if loss > diverge_th * self.best_loss:\n",
        "                print(\"Stopping early, the loss has diverged\")\n",
        "                break\n",
        "\n",
        "        print(\"Learning rate search finished. See the graph with {finder_name}.plot()\")\n",
        "\n",
        "    def _train_batch(self, inputs, labels):\n",
        "        # Set model to training mode\n",
        "#         self.model.train()\n",
        "\n",
        "        # Move data to the correct device\n",
        "        inputs = inputs.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        # Forward pass\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        loss = self.criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def _validate(self, dataloader):\n",
        "        # Set model to evaluation mode and disable gradient computation\n",
        "        running_loss = 0\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloader:\n",
        "                # Move data to the correct device\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                # Forward pass and loss computation\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        return running_loss / len(dataloader.dataset)\n",
        "\n",
        "    def plot(self, skip_start=10, skip_end=5, log_lr=True):\n",
        "        \"\"\"Plots the learning rate range test.\n",
        "\n",
        "        Arguments:\n",
        "            skip_start (int, optional): number of batches to trim from the start.\n",
        "                Default: 10.\n",
        "            skip_end (int, optional): number of batches to trim from the start.\n",
        "                Default: 5.\n",
        "            log_lr (bool, optional): True to plot the learning rate in a logarithmic\n",
        "                scale; otherwise, plotted in a linear scale. Default: True.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if skip_start < 0:\n",
        "            raise ValueError(\"skip_start cannot be negative\")\n",
        "        if skip_end < 0:\n",
        "            raise ValueError(\"skip_end cannot be negative\")\n",
        "\n",
        "        # Get the data to plot from the history dictionary. Also, handle skip_end=0\n",
        "        # properly so the behaviour is the expected\n",
        "        lrs = self.history[\"lr\"]\n",
        "        losses = self.history[\"loss\"]\n",
        "        if skip_end == 0:\n",
        "            lrs = lrs[skip_start:]\n",
        "            losses = losses[skip_start:]\n",
        "        else:\n",
        "            lrs = lrs[skip_start:-skip_end]\n",
        "            losses = losses[skip_start:-skip_end]\n",
        "\n",
        "        # Plot loss as a function of the learning rate\n",
        "        plt.plot(lrs, losses)\n",
        "        if log_lr:\n",
        "            plt.xscale(\"log\")\n",
        "        plt.xlabel(\"Learning rate\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class LinearLR(_LRScheduler):\n",
        "    \"\"\"Linearly increases the learning rate between two boundaries over a number of\n",
        "    iterations.\n",
        "\n",
        "    Arguments:\n",
        "        optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
        "        end_lr (float, optional): the initial learning rate which is the lower\n",
        "            boundary of the test. Default: 10.\n",
        "        num_iter (int, optional): the number of iterations over which the test\n",
        "            occurs. Default: 100.\n",
        "        last_epoch (int): the index of last epoch. Default: -1.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
        "        self.end_lr = end_lr\n",
        "        self.num_iter = num_iter\n",
        "        super(LinearLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        curr_iter = self.last_epoch + 1\n",
        "        r = curr_iter / self.num_iter\n",
        "        return [base_lr + r * (self.end_lr - base_lr) for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "class ExponentialLR(_LRScheduler):\n",
        "    \"\"\"Exponentially increases the learning rate between two boundaries over a number of\n",
        "    iterations.\n",
        "\n",
        "    Arguments:\n",
        "        optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
        "        end_lr (float, optional): the initial learning rate which is the lower\n",
        "            boundary of the test. Default: 10.\n",
        "        num_iter (int, optional): the number of iterations over which the test\n",
        "            occurs. Default: 100.\n",
        "        last_epoch (int): the index of last epoch. Default: -1.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
        "        self.end_lr = end_lr\n",
        "        self.num_iter = num_iter\n",
        "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        curr_iter = self.last_epoch + 1\n",
        "        r = curr_iter / self.num_iter\n",
        "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
        "\n",
        "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
        "\n",
        "root = './data'\n",
        "if not os.path.exists(root):\n",
        "    os.mkdir(root)\n",
        "train_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\n",
        "test_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "                 dataset=train_set,\n",
        "                 batch_size=batch_size,\n",
        "                 shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "                dataset=test_set,\n",
        "                batch_size=batch_size,\n",
        "shuffle=True)\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 500)\n",
        "        self.fc2 = nn.Linear(500, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "num_epochs = 2\n",
        "random_sample_size = 200\n",
        "\n",
        "\n",
        "# Hyper-parameters \n",
        "input_size = (32,32,3)\n",
        "hidden_size = 100\n",
        "num_classes = 5\n",
        "learning_rate = .0001\n",
        "# Device configuration\n",
        "device = 'cpu'\n",
        "\n",
        "model = NeuralNet().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "lr_finder = LRFinder(model, optimizer, criterion, device=\"cpu\")\n",
        "lr_finder.range_test(train_loader, end_lr=100, num_iter=100)\n",
        "lr_finder.plot()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr_finder.history['lr'][0])  \n",
        "print(lr_finder.history['lr'])\n",
        "\n",
        "predicted_test = []\n",
        "labels_l = []\n",
        "actual_values = []\n",
        "predicted_values = []\n",
        "\n",
        "N = len(train_loader)\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # Move tensors to the configured device\n",
        "#         images = images.reshape(-1, 50176).to(device)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        predicted = outputs.data.max(1)[1]\n",
        "        predicted_test.append(predicted.cpu().numpy())\n",
        "        labels_l.append(labels.cpu().numpy())\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    predicted_values.append(np.concatenate(predicted_test).ravel())\n",
        "    actual_values.append(np.concatenate(labels_l).ravel())\n",
        "\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "    print('training accuracy : ', 100 * len((np.where(np.array(predicted_values[0])==(np.array(actual_values[0])))[0])) / len(actual_values[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Bz2j_bJn3d9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "1e99761f-ae9a-4d3c-af9c-f7a3efc9317d"
      },
      "source": [
        "# Loss and Optimizer\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0000001), metrics=['accuracy'])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "# Store Training Results\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, verbose=1, mode='auto')\n",
        "callback_list = [early_stopping]\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train_lt5, y_train_lt5, batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle=True,\n",
        "          validation_data=(x_test_lt5, y_test_lt5), callbacks=callback_list)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 5000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 5s 213us/step - loss: 1.2118 - acc: 0.4885 - val_loss: 1.0037 - val_acc: 0.6008\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 5s 191us/step - loss: 0.9749 - acc: 0.6043 - val_loss: 0.8920 - val_acc: 0.6450\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 5s 195us/step - loss: 0.8939 - acc: 0.6412 - val_loss: 0.8574 - val_acc: 0.6582\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 5s 186us/step - loss: 0.8440 - acc: 0.6647 - val_loss: 0.8091 - val_acc: 0.6780\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 5s 182us/step - loss: 0.8027 - acc: 0.6830 - val_loss: 0.8083 - val_acc: 0.6856\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 5s 183us/step - loss: 0.7604 - acc: 0.7007 - val_loss: 0.8386 - val_acc: 0.6722\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 5s 182us/step - loss: 0.7337 - acc: 0.7153 - val_loss: 0.7975 - val_acc: 0.6930\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 5s 183us/step - loss: 0.7060 - acc: 0.7244 - val_loss: 0.7863 - val_acc: 0.6874\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 5s 182us/step - loss: 0.6818 - acc: 0.7336 - val_loss: 0.7694 - val_acc: 0.7024\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 5s 184us/step - loss: 0.6553 - acc: 0.7441 - val_loss: 0.8149 - val_acc: 0.6886\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 5s 181us/step - loss: 0.6345 - acc: 0.7538 - val_loss: 0.7805 - val_acc: 0.7054\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 5s 184us/step - loss: 0.6121 - acc: 0.7619 - val_loss: 0.8175 - val_acc: 0.6946\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 5s 183us/step - loss: 0.5976 - acc: 0.7681 - val_loss: 0.8081 - val_acc: 0.7082\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 5s 187us/step - loss: 0.5651 - acc: 0.7784 - val_loss: 0.8620 - val_acc: 0.6848\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 5s 185us/step - loss: 0.5441 - acc: 0.7859 - val_loss: 0.8105 - val_acc: 0.7062\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 5s 182us/step - loss: 0.5208 - acc: 0.7986 - val_loss: 0.8846 - val_acc: 0.6880\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 5s 186us/step - loss: 0.5050 - acc: 0.8034 - val_loss: 0.8480 - val_acc: 0.6938\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 5s 182us/step - loss: 0.4872 - acc: 0.8093 - val_loss: 0.8988 - val_acc: 0.6932\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 5s 182us/step - loss: 0.4753 - acc: 0.8144 - val_loss: 0.8540 - val_acc: 0.7082\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 5s 181us/step - loss: 0.4439 - acc: 0.8292 - val_loss: 0.9292 - val_acc: 0.7008\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f74a09f4128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "woTfNst_ynRG"
      },
      "source": [
        "### 4. In the model which was built above (for classification of classes 0-4 in CIFAR10), make only the dense layers to be trainable and conv layers to be non-trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o_VCDB3Byb1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "3cd848b6-860b-4097-adc7-a8f73323fa0c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 28, 28, 32)        2432      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 24, 24, 16)        12816     \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 10, 10, 8)         1160      \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 64)                51264     \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 5)                 325       \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 67,997\n",
            "Trainable params: 67,997\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD_xHt-pNEoH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "outputId": "6659d2e5-6508-4661-ff1c-79d65e77f2af"
      },
      "source": [
        "for layers in model.layers:\n",
        "    print(layers.name)\n",
        "    if('dense' not in layers.name):\n",
        "        layers.trainable = False\n",
        "        print(layers.name + 'is not trainable\\n')\n",
        "    if('dense' in layers.name):\n",
        "        print(layers.name + ' is trainable\\n')"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2d_7\n",
            "conv2d_7is not trainable\n",
            "\n",
            "activation_11\n",
            "activation_11is not trainable\n",
            "\n",
            "conv2d_8\n",
            "conv2d_8is not trainable\n",
            "\n",
            "activation_12\n",
            "activation_12is not trainable\n",
            "\n",
            "max_pooling2d_3\n",
            "max_pooling2d_3is not trainable\n",
            "\n",
            "conv2d_9\n",
            "conv2d_9is not trainable\n",
            "\n",
            "flatten_3\n",
            "flatten_3is not trainable\n",
            "\n",
            "dense_7\n",
            "dense_7 is trainable\n",
            "\n",
            "activation_13\n",
            "activation_13is not trainable\n",
            "\n",
            "dropout_3\n",
            "dropout_3is not trainable\n",
            "\n",
            "dense_8\n",
            "dense_8 is trainable\n",
            "\n",
            "activation_14\n",
            "activation_14is not trainable\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1-uUPqWpyeyX"
      },
      "source": [
        "### 5. Utilize the the model trained on CIFAR 10 (classes 0 to 4) to classify the classes 5 to 9 of CIFAR 10  (Use Transfer Learning) <br>\n",
        "Achieve an accuracy of more than 85% on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "szHjJgDvyfCt",
        "colab": {}
      },
      "source": [
        "x_train_gt5 = np.asarray(x_train_gt5).reshape((-1, 32, 32, 3))\n",
        "x_test_gt5 = np.asarray(x_test_gt5).reshape((-1, 32, 32, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACZ_Y07ITcJ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "fdc76769-a582-4c0b-b59c-6c1e182c59c2"
      },
      "source": [
        "print(x_train_gt5.shape)\n",
        "print(y_train_gt5.shape)\n",
        "print(x_test_gt5.shape)\n",
        "print(y_test_gt5.shape)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 32, 32, 3)\n",
            "(25000, 10)\n",
            "(5000, 32, 32, 3)\n",
            "(5000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo1ODD4KQtFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trans_model = Sequential(model.layers[:6])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc2nuFDeQxiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trans_model.add(Flatten())\n",
        "#trans_model.add(Dense(128))\n",
        "#trans_model.add(Activation('relu'))\n",
        "#trans_model.add(Dense(10))\n",
        "#trans_model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7X0Zs8NQ30q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trans_model.summary()\n",
        "#trans_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rCro73hd2nz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4y49gpQOxa4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "a80cb295-aeac-4501-ebb1-d716e3671a3d"
      },
      "source": [
        "print(x_train_gt5.shape)\n",
        "print(y_train_gt5.shape)\n",
        "print(x_test_gt5.shape)\n",
        "print(y_test_gt5.shape)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 32, 32, 3)\n",
            "(25000, 10)\n",
            "(5000, 32, 32, 3)\n",
            "(5000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdoduZUmPGd7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "outputId": "746e3729-71fa-4f5d-f48c-7d8807638df2"
      },
      "source": [
        "# Train the model\n",
        "#trans_model.fit(x_train_gt5, y_train_gt5, nb_epoch=10, shuffle=True, batch_size=100, \n",
        "#                validation_data=(x_test_gt5, y_test_gt5), verbose=2)\n",
        "model.fit(x_train_gt5, y_train_gt5, batch_size=BATCH_SIZE, nb_epoch=EPOCHS, \n",
        "          validation_data=(x_test_gt5, y_test_gt5), callbacks=callback_list)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 5000 samples\n",
            "Epoch 1/20\n",
            " 1184/25000 [>.............................] - ETA: 3s - loss: 0.5054 - acc: 0.8404"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 4s 156us/step - loss: 0.5246 - acc: 0.8313 - val_loss: 0.8840 - val_acc: 0.7108\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 4s 155us/step - loss: 0.5130 - acc: 0.8371 - val_loss: 0.8825 - val_acc: 0.7126\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 4s 154us/step - loss: 0.5031 - acc: 0.8402 - val_loss: 0.8898 - val_acc: 0.7112\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 4s 153us/step - loss: 0.4898 - acc: 0.8439 - val_loss: 0.9277 - val_acc: 0.7098\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 4s 155us/step - loss: 0.4769 - acc: 0.8502 - val_loss: 0.9242 - val_acc: 0.7070\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 4s 154us/step - loss: 0.4687 - acc: 0.8521 - val_loss: 0.9257 - val_acc: 0.7020\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 4s 156us/step - loss: 0.4645 - acc: 0.8540 - val_loss: 0.9344 - val_acc: 0.7026\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 4s 156us/step - loss: 0.4620 - acc: 0.8555 - val_loss: 0.9617 - val_acc: 0.7002\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 4s 154us/step - loss: 0.4497 - acc: 0.8620 - val_loss: 0.9378 - val_acc: 0.7052\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 4s 154us/step - loss: 0.4417 - acc: 0.8619 - val_loss: 0.9667 - val_acc: 0.7014\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 4s 154us/step - loss: 0.4375 - acc: 0.8643 - val_loss: 0.9640 - val_acc: 0.7038\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 4s 153us/step - loss: 0.4329 - acc: 0.8672 - val_loss: 0.9643 - val_acc: 0.7044\n",
            "Epoch 00012: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f74b95974e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FU-HwvIdH0M-"
      },
      "source": [
        "## Sentiment analysis <br> \n",
        "\n",
        "The objective of the second problem is to perform Sentiment analysis from the tweets data collected from the users targeted at various mobile devices.\n",
        "Based on the tweet posted by a user (text), we will classify if the sentiment of the user targeted at a particular mobile device is positive or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RZQ1M8-sGhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAQDiZHRH0M_"
      },
      "source": [
        "### 6. Read the dataset (tweets.csv) and drop the NA's while reading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3eXGIe-SH0NA",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('./tweets.csv', encoding = \"ISO-8859-1\").dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CWeWe1eJH0NF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1648c06-853a-4c61-bf5b-80b6875c84fc"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3291, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7kX-WoJDH0NV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "43e8d559-87ff-4aa7-a920-2a144efd651c"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>emotion_in_tweet_is_directed_at</th>\n",
              "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
              "      <td>iPhone</td>\n",
              "      <td>Negative emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
              "      <td>iPad or iPhone App</td>\n",
              "      <td>Positive emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
              "      <td>iPad</td>\n",
              "      <td>Positive emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
              "      <td>iPad or iPhone App</td>\n",
              "      <td>Negative emotion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
              "      <td>Google</td>\n",
              "      <td>Positive emotion</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          tweet_text  ... is_there_an_emotion_directed_at_a_brand_or_product\n",
              "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  ...                                   Negative emotion\n",
              "1  @jessedee Know about @fludapp ? Awesome iPad/i...  ...                                   Positive emotion\n",
              "2  @swonderlin Can not wait for #iPad 2 also. The...  ...                                   Positive emotion\n",
              "3  @sxsw I hope this year's festival isn't as cra...  ...                                   Negative emotion\n",
              "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  ...                                   Positive emotion\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OGWB3P2WH0NY"
      },
      "source": [
        "### Consider only rows having Positive emotion and Negative emotion and remove other rows from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bdgA_8N2H0NY",
        "colab": {}
      },
      "source": [
        "data = data[(data['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Positive emotion') | (data['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Negative emotion')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Jlu-reIH0Na",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e5ccd06-f1b1-44fc-9489-1a0315d377b8"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3191, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SotCRvkDH0Nf"
      },
      "source": [
        "### 7. Represent text as numerical data using `CountVectorizer` and get the document term frequency matrix\n",
        "\n",
        "#### Use `vect` as the variable name for initialising CountVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YcbkY4sgH0Ng",
        "colab": {}
      },
      "source": [
        "X = data['tweet_text']\n",
        "y = data['is_there_an_emotion_directed_at_a_brand_or_product']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KyXtZGr-H0Nl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z4LUM-XPH0Nn",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect = CountVectorizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aIdZYxJtH0Nq",
        "colab": {}
      },
      "source": [
        "vect.fit(X_train)\n",
        "X_train_dtm = vect.transform(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlyHXsHSn9ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vect.fit(X_test)\n",
        "X_test_dtm = vect.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5pxd5fSHH0Nt"
      },
      "source": [
        "### 8. Find number of different words in vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p1DQ2LdNH0Nu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6700746-a3c4-4ac4-a1c0-60a682323ff5"
      },
      "source": [
        "# store the vocabulary of X_train\n",
        "X_train_tokens = vect.get_feature_names()\n",
        "len(X_train_tokens)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4900"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7BnXJt3oi5X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5bd5a515-f8f8-484c-bc87-649855d84bd9"
      },
      "source": [
        "# store the vocabulary of X_test\n",
        "X_test_tokens = vect.get_feature_names()\n",
        "len(X_test_tokens)"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4900"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dwtgjTBeH0Ny"
      },
      "source": [
        "#### Tip: To see all available functions for an Object use dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2n_iCcTNH0N0",
        "colab": {}
      },
      "source": [
        "dir(cv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TdmYB6qlMu-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79105c5c-9be4-4978-b34f-bd35106fb790"
      },
      "source": [
        "dir(vect)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setstate__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_char_ngrams',\n",
              " '_char_wb_ngrams',\n",
              " '_check_stop_words_consistency',\n",
              " '_check_vocabulary',\n",
              " '_count_vocab',\n",
              " '_get_param_names',\n",
              " '_get_tags',\n",
              " '_limit_features',\n",
              " '_more_tags',\n",
              " '_sort_features',\n",
              " '_stop_words_id',\n",
              " '_validate_custom_analyzer',\n",
              " '_validate_params',\n",
              " '_validate_vocabulary',\n",
              " '_white_spaces',\n",
              " '_word_ngrams',\n",
              " 'analyzer',\n",
              " 'binary',\n",
              " 'build_analyzer',\n",
              " 'build_preprocessor',\n",
              " 'build_tokenizer',\n",
              " 'decode',\n",
              " 'decode_error',\n",
              " 'dtype',\n",
              " 'encoding',\n",
              " 'fit',\n",
              " 'fit_transform',\n",
              " 'fixed_vocabulary_',\n",
              " 'get_feature_names',\n",
              " 'get_params',\n",
              " 'get_stop_words',\n",
              " 'input',\n",
              " 'inverse_transform',\n",
              " 'lowercase',\n",
              " 'max_df',\n",
              " 'max_features',\n",
              " 'min_df',\n",
              " 'ngram_range',\n",
              " 'preprocessor',\n",
              " 'set_params',\n",
              " 'stop_words',\n",
              " 'stop_words_',\n",
              " 'strip_accents',\n",
              " 'token_pattern',\n",
              " 'tokenizer',\n",
              " 'transform',\n",
              " 'vocabulary',\n",
              " 'vocabulary_']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ShA6D8jKH0N5"
      },
      "source": [
        "### Find out how many Positive and Negative emotions are there.\n",
        "\n",
        "Hint: Use value_counts on that column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7LAl5pzH0N6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "34627071-587e-4aa4-a191-3790abfdae66"
      },
      "source": [
        "pd.value_counts(data['is_there_an_emotion_directed_at_a_brand_or_product'])"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Positive emotion    2672\n",
              "Negative emotion     519\n",
              "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IUvgj0FoH0N9"
      },
      "source": [
        "###  Change the labels for Positive and Negative emotions as 1 and 0 respectively and store in a different column in the same dataframe named 'label'\n",
        "\n",
        "Hint: use map on that column and give labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YftKwFv7H0N9",
        "colab": {}
      },
      "source": [
        "data['label'] = data.is_there_an_emotion_directed_at_a_brand_or_product.map({'Positive emotion':1, 'Negative emotion':0})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YErwYLCH0N_"
      },
      "source": [
        "### 9. Define the feature set (independent variable or X) to be `text` column and `labels` as target (or dependent variable)  and divide into train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lNkwrGgEH0OA",
        "colab": {}
      },
      "source": [
        "# already done in step 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q5nlCuaaH0OD"
      },
      "source": [
        "## 10. **Predicting the sentiment:**\n",
        "\n",
        "\n",
        "### Use Naive Bayes and Logistic Regression and their accuracy scores for predicting the sentiment of the given text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2AbVYssaH0OE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80b910f9-45c4-4ea9-c4ca-cd3d74b6f6fe"
      },
      "source": [
        "# use Naive Bayes to predict the sentiment\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_dtm, y_train)\n",
        "y_pred_class = nb.predict(X_test_dtm)\n",
        "\n",
        "# calculate accuracy\n",
        "print (metrics.accuracy_score(y_test, y_pred_class))"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8671679197994987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzecqnKDyI26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "cd2411ea-1f15-4930-c713-fbd29ed4f112"
      },
      "source": [
        "# use logistic regression with text column only\n",
        "logreg = LogisticRegression(C=1e9)\n",
        "logreg.fit(X_train_dtm, y_train)\n",
        "y_pred_class = logreg.predict(X_test_dtm)\n",
        "print (metrics.accuracy_score(y_test, y_pred_class))"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8533834586466166\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sw-0B33tH0Ox"
      },
      "source": [
        "## 11. Create a function called `tokenize_predict` which can take count vectorizer object as input and prints the accuracy for x (text) and y (labels)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "okCTOs1TH0Oy",
        "colab": {}
      },
      "source": [
        "def tokenize_test(vect):\n",
        "    vect.fit(X_train)\n",
        "    X_train_dtm = vect.transform(X_train)\n",
        "    #x_train_dtm = vect.fit_transform(x_train)\n",
        "    print('Features: ', X_train_dtm.shape[1])\n",
        "    X_test_dtm = vect.transform(X_test)\n",
        "    nb = MultinomialNB()\n",
        "    nb.fit(X_train_dtm, y_train)\n",
        "    y_pred_class = nb.predict(X_test_dtm)\n",
        "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JxZ8jfPEH0O0"
      },
      "source": [
        "### Create a count vectorizer function which includes n_grams = 1,2  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kdCyAN_IH0O0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b915ff36-539b-4be5-96b3-7258b3827ca9"
      },
      "source": [
        "# include 1-grams and 2-grams\n",
        "vect = CountVectorizer(ngram_range=(1, 2))\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  24650\n",
            "Accuracy:  0.8759398496240601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "axepytmgH0O4"
      },
      "source": [
        "### 12. Create a count vectorizer function with stopwords = 'english'  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HToGkq7vH0O4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ca49293a-b7fd-457a-dd46-6cae0cf7f58d"
      },
      "source": [
        "vect = CountVectorizer(stop_words='english')\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  4661\n",
            "Accuracy:  0.8659147869674185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iOIlJRxoH0O7"
      },
      "source": [
        "### 13. Create a count vectorizer function with stopwords = 'english' and max_features =300  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6fUhff-oH0O8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8d59e1e7-33d8-4dfd-e305-45d43c30aec9"
      },
      "source": [
        "vect = CountVectorizer(stop_words='english', max_features=300)\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  300\n",
            "Accuracy:  0.8233082706766918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S2KZNWVkH0PA"
      },
      "source": [
        "### 14. Create a count vectorizer function with n_grams = 1,2  and max_features = 15000  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3v9XD082H0PB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c0f49ae8-3743-473a-ab39-410bf8057d02"
      },
      "source": [
        "vect = CountVectorizer(ngram_range=(1, 2), max_features=15000)\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  15000\n",
            "Accuracy:  0.8771929824561403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "We3JK_SRH0PO"
      },
      "source": [
        "### 15. Create a count vectorizer function with n_grams = 1,2  and include terms that appear at least 2 times (min_df = 2)  and pass it to tokenize_predict function to print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fUHrfDCyH0PP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bcfdd12b-6a41-4cf2-addf-889bc05eb5f6"
      },
      "source": [
        "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
        "tokenize_test(vect)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  7925\n",
            "Accuracy:  0.8796992481203008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwC6001N7fjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}